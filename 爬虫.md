# 案例

代码没有分行显示，可以  ctrl+alt+L 来实现分行处理

## 爬取对应词条的搜索页面

```python
import requests

# 需要UA伪装
# 将对应的User-Agent封装到一个字典当中
# 这个是在Network中检查的，在页面响应的瞬间可以查看
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66'
}
kw = str(input('enter:'))
url = 'https://www.sogou.com/web?query={0}'.format(kw)
response = requests.get(url=url, headers=headers)
page_txt = response.text
with open('./save.html', 'w', encoding='utf-8') as fp:
    fp.write(page_txt)
    # 对于爬取出来之后的代码没有分行显示，可以  ctrl+alt+l 来实现分行处理
    print('end')
```

`requests.get(url,param,headers)`三个参数，其中param可以对应的是**query**的值，也是可以保存在字典当中的

## 爬取翻译网站的文本框中的数据

一般来说定位到XHR中，可以找到该网站所请求的数据包

![image-20201229155100078](C:\Users\梅桂楠\AppData\Roaming\Typora\typora-user-images\image-20201229155100078.png)

明显是一个**POST**的请求，所以需要携带参数

![image-20201229155247743](C:\Users\梅桂楠\AppData\Roaming\Typora\typora-user-images\image-20201229155247743.png)

![image-20201229155430971](C:\Users\梅桂楠\AppData\Roaming\Typora\typora-user-images\image-20201229155430971.png)

**Content-Type：**表示服务端响应为客户端的数据类型，表示相应的数据是一组**JSON数据**

![image-20201229155542856](C:\Users\梅桂楠\AppData\Roaming\Typora\typora-user-images\image-20201229155542856.png)

就是服务器端响应回来的数据

```python
import requests
import json

# 需要UA伪装
# 将对应的User-Agent封装到一个字典当中
# 这个是在Network中检查的，在页面响应的瞬间可以查看，查看了一下，百度和搜狗的这一项是一样的
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66'
}
# 每个网页的url都不同
post_url = 'https://fanyi.baidu.com/sug'
# post请求参数处理，和get是一样的
word = input('enter the word:')
data = {
    'kw': word
}
# 获取请求，还是需要UA伪装
response = requests.post(url=post_url, data=data, headers=headers)
# 获取响应的数据：json()返回的是一个对象obj，检查发现是一个字典对象（如果确认响应数据类型是json类型才可使用）
dic_obj = response.json()
# 持久化存储
with open('./save.json', 'w', encoding='utf-8') as fp:
    # 不能使用ascii码
    json.dump(dic_obj, fp, ensure_ascii=False)
    # 对于爬取出来之后的代码没有分行显示，可以  ctrl+alt+l 来实现分行处理
    print('end')
```

这个相对应的在**有道翻译网页**上是行不通的

## 爬取肯德基位置信息

```python
import requests
import json

# 需要UA伪装
# 将对应的User-Agent封装到一个字典当中
# 这个是在Network中检查的，在页面响应的瞬间可以查看，查看了一下，百度和搜狗的这一项是一样的
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66'
}
# 每个网页的url都不同
cname = str(input('enter:'))
url = 'http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=cname'
data = {
    'cname': cname,
    'pageIndex': '1',   # 从第一个位置开始爬取
    'pageSize': '10'    # 只爬取十个
}
# post请求参数处理，和get是一样的
# 获取请求，还是需要UA伪装
response = requests.post(url=url, data=data, headers=headers)
# 获取响应的数据：json()返回的是一个对象obj，检查发现是一个字典对象（如果确认响应数据类型是json类型才可使用）
dic_obj = response.text
# 持久化存储
with open('./save.json', 'w', encoding='utf-8') as fp:
    # 不能使用ascii码
    json.dump(dic_obj, fp, ensure_ascii=False)
    # 对于爬取出来之后的代码没有分行显示，可以  ctrl+alt+l 来实现分行处理
    print('end')
```

## 爬取化妆品数据

这个主要在于还爬取了化妆品公司的详细信息，有一个多次跳动，也就是对**JSON**文件字段的提取

```python
import requests
import json

# 需要UA伪装
# 将对应的User-Agent封装到一个字典当中
# 这个是在Network中检查的，在页面响应的瞬间可以查看，查看了一下，百度和搜狗的这一项是一样的
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66'
}
# 每个网页的url都不同
url = 'http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do?method=getXkzsList'
data = {
    'on': 'true',
    'page': '1',
    'pageSize': '15',
    'productName': '',
    'conditionType': '1',
    'applyname': '',
    'applysn': ''
}
# post请求参数处理，和get是一样的
# 获取请求，还是需要UA伪装
response = requests.post(url=url, data=data, headers=headers)
# 获取响应的数据：json()返回的是一个对象obj，检查发现是一个字典对象（如果确认响应数据类型是json类型才可使用）
dic_obj = response.json()
# 持久化存储
with open('./save.json', 'w', encoding='utf-8') as fp:
    # 不能使用ascii码
    json.dump(dic_obj, fp, ensure_ascii=False)
    # 对于爬取出来之后的代码没有分行显示，可以  ctrl+alt+l 来实现分行处理
    print('end')

# 从json文件中获取ID的数据
id_list = []
all_data_list = []
for dic in dic_obj['list']:
    id_list.append(dic['ID'])

# 获取详情页的数据
urls = 'http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do?method=getXkzsById'
for ids in id_list:
    datas = {
        'id': ids
    }
    responses = requests.post(url=urls, data=datas, headers=headers)
    # 获取响应的数据：json()返回的是一个对象obj，检查发现是一个字典对象（如果确认响应数据类型是json类型才可使用）
    dic_objs = responses.json()
    all_data_list.append(dic_objs)
with open('./saves.json', 'w', encoding='utf-8') as fp:
    # 不能使用ascii码
    json.dump(all_data_list, fp, ensure_ascii=False)
    # 对于爬取出来之后的代码没有分行显示，可以  ctrl+alt+l 来实现分行处理
    print('end')
```

# 基础

.txt：返回字符串形式的数据

.json：返回对象类型的数据

.content：返回二进制格式的图片数据

## 聚焦爬虫

爬取页面中指定的页面内容

**爬虫流程**

- 指定URL
- 发起请求
- 获取响应数据
- 数据解析
- 持久化存储

## 数据解析

通过获取对应的**标签**来获取对应的数据，也可以从**标签**对应的**属性**中获取对应的数据

1 进行标签的定位

2 对标签或者标签对应的（属性）中存储的数据值进行存储（数据解析）

### 正则

![img](https://book.apeland.cn/media/images/2019/04/15/snip20190415_21.png)

#### 爬取糗事百科中糗图板块下的图片

一般来说先获取整张页面，然后使用聚焦爬虫，进行数据解析

![image-20201230210938226](C:\Users\梅桂楠\AppData\Roaming\Typora\typora-user-images\image-20201230210938226.png)

可以看出在前端标签的属性字段中存放着这个图片的**url地址：**<img src="">

所以可以在此获取所有图片的url，然后再批量的对这些url发送请求获取图片数据

可以看出：

```html
<div>
    <a>
    <img src=""/>
    </a>
</div>
```

所以首先需要找到所属的**div**标签，通常来说都是div标签套div标签

配置正则：`ex='<div class="thumb">.*?<img src="(.*?)" alt.*?</div>`

让正则作用到字符串当中：`re.findall(ex, img_text, re.S)`

**此时只能下载第一页的图片：**

```python
import requests
import os
import re

# 创建一个文件夹来保存所有爬取到的数据
if not os.path.exists('./img_test'):
    os.mkdir('./img_test')

# 需要UA伪装
# 将对应的User-Agent封装到一个字典当中
# 这个是在Network中检查的，在页面响应的瞬间可以查看，查看了一下，百度和搜狗的这一项是一样的
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66'
}
# 首先请求整个页面
url_text = 'https://www.qiushibaike.com/imgrank/'
# 使用通用爬虫对一整张页面进行爬取
img_text = requests.get(url=url_text, headers=headers).text
# 使用聚焦爬虫将页面中的请求进行数据解析
# 配置正则函数
ex = '<div class="thumb">.*?<img src="(.*?)" alt.*?</div>'
# 将正则作用于一个字符串中，re.S表示单行匹配，re.M表示多行匹配，返回的是一个列表
img_src_list = re.findall(ex, img_text, re.S)
for img_src in img_src_list:
    # 拼接出一个完整的图片的url地址
    img_url = 'https:' + img_src
    # content返回的是二进制形式的图片数据
    img = requests.get(url=img_url, headers=headers).content
    # 生成图片名称，从img_src根据‘/’来进行切割
    img_name = img_src.split('/')[-1]
    # 图片存储的路径
    img_path = './img_test/' + img_name
    # 存储的是二进制数据，所以是‘wb’
    with open(img_path, 'wb') as fp:
        fp.write(img)
        print(img_name + '下载成功！')
```

**这个可以获取不同页码的页面：**

```python
import requests
import os
import re

# 创建一个文件夹来保存所有爬取到的数据
if not os.path.exists('./img_test'):
    os.mkdir('./img_test')

# 需要UA伪装
# 将对应的User-Agent封装到一个字典当中
# 这个是在Network中检查的，在页面响应的瞬间可以查看，查看了一下，百度和搜狗的这一项是一样的
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66'
}
# 首先请求整个页面
# 想请求不同页码的数据，所以可以拼接以下不同页面的url
# url_all = []
# for page in range(1, 2):
    # url_all.append('https://www.qiushibaike.com/imgrank/page/{0}/'.format(page))
for page in range(1, 3):
    url_text = 'https://www.qiushibaike.com/imgrank/page/{0}/'.format(page)
    # 使用通用爬虫对一整张页面进行爬取
    img_text = requests.get(url=url_text, headers=headers).text
    # 使用聚焦爬虫将页面中的请求进行数据解析
    # 配置正则函数
    ex = '<div class="thumb">.*?<img src="(.*?)" alt.*?</div>'
    # 将正则作用于一个字符串中，re.S表示单行匹配，re.M表示多行匹配，返回的是一个列表
    img_src_list = re.findall(ex, img_text, re.S)
    for img_src in img_src_list:
        # 拼接出一个完整的图片的url地址
        img_url = 'https:' + img_src
        # content返回的是二进制形式的图片数据
        img = requests.get(url=img_url, headers=headers).content
        # 生成图片名称，从img_src根据‘/’来进行切割
        img_name = img_src.split('/')[-1]
        # 图片存储的路径
        img_path = './img_test/' + img_name
        # 存储的是二进制数据，所以是‘wb’
        with open(img_path, 'wb') as fp:
            fp.write(img)
            print(img_name + '下载成功！')
```

### bs4

**只能应用于python语言之中**

**数据解析原理：**

- 标签定位
- 提取标签、标签属性中存储的数据值

**bs4数据解析原理：**

- 实例化BeautifulSoup对象，并且将页面源码数据加载到该对象中
- 通过调用BeautifulSoup对象中的属性或者方法进行标签定位和数据提取

**实例化BeautifulSoup对象：**

- `from bs4 import BeautifulSoup`

- 对象的实例化

  - 将本地的html文档中的数据加载到该对象之中

    `BeautifulSoup（文件名，'lxml'）`

    ```python
    fp = open('./test.html', 'r', encoding='utf-8')
    soup = BeautifulSoup(fp, 'lxml')
    ```

  - 将互联网上获取的页面源码加载到该对象中

    ```python
    page_text = requests.get(url=url, headers=headers).text
    soup = BeautifulSoup(page_text, 'lxml')
    # 当然不一定是get，只需要获取到页面的源码就可以了
    ```

- 提供应用于数据解析的方法和属性

  - `soup.标签名`    只会得到整个源码中的**第一次**出现这个标签名字地方的数据

  - `soup.find('标签名')`返回的也是第一次出现这个标签名的地方

    ​	`soup.find('标签名', class_='属性名')`返回的就是**class属性**所对应的**标签**

  - `soup.find_all('标签名')`返回的是出现这个标签名的所有的地方，以**列表**的形式返回，当然这个也可以进行属性定位

  - `soup.select('选择器样式+选择器名')`比如**类选择器**就是`.class属性名`，**标签选择器什么都不加**，返回的是一个列表

    ```python
    soup.select('.tang > url > li > a')[0]
    # 可以使用这种方法来寻找层级关系的标签，返回的是列表形式，>表示下一个层级，[0]表示返回的是<a>标签中的第一个标签
    
    soup.select('.tang > url a')[0]
    # 空格表示多个层级，>表示一个层级
    ```

- 获取标签之间的文本数据

  ```python
  soup.select('.tang > url a')[0].text
  soup.select('.tang > url a')[0].string	
  soup.select('.tang > url a')[0].get_text()
  # 三个都是可以获取到该标签中的文本数据
  
  text/get_text()可以获取到标签当中所有的文本数据
  string只能获取到该标签中直系的文本内容
  # 获取直系的意思就是，如果这个标签下面没有文本，但是其下属标签中有文本，也不会被获取
  ```

- 获取标签中的属性值

  `soup.标签名['属性名称']`

  ```python
  soup.select('.tang > url a')[0]['href']
  # 获取到的就是href这个属性字段所对应的数据
  ```

**爬取三国演义小说中的所有的标题**

```html
这些标题都存储在以下的标签内
<body>
    <div id="main">
        <div id="main_left">
            <div class="card bookmark-list">
                <div class="book-mulu">
                    <ul>
                        <li>
                        	<a href="/book/sanguoyanyi/1.html">第一回·宴桃园豪杰三结义 斩黄巾英雄首立功</a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
```

```python
from bs4 import BeautifulSoup
import requests

# 需要UA伪装
# 将对应的User-Agent封装到一个字典当中
# 这个是在Network中检查的，在页面响应的瞬间可以查看，查看了一下，百度和搜狗的这一项是一样的
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66'
}

url = 'https://www.shicimingju.com/book/sanguoyanyi.html'
page_text = requests.get(url=url, headers=headers).text
soup = BeautifulSoup(page_text, 'lxml')
# # 获取每一章内容所对应的url
# page_contents = soup.select('.book-mulu > ul a')[0]['href']
# url_contents = 'https://www.shicimingju.com' + page_contents
# # 获取每一章的标题
# page_totalText = soup.select('.book-mulu > ul')[0].text

# 提前打开建立好一个文件，不然在for循环中都会重新建立新的文件
fp = open('./sanguo.txt', 'w', encoding='utf-8')

li_list = soup.select('.book-mulu > ul > li')
for li in li_list:
    title_list = li.a.text
    url_list = 'https://www.shicimingju.com' + li.a['href']
    page_content = requests.get(url=url_list, headers=headers).text
    # 解析出详情页中相关页的内容
    page_soup = BeautifulSoup(page_content, 'lxml')
    div_tag = page_soup.find('div', class_='chapter_content')
    # 获取解析到的章节中的内容
    content = div_tag.text
    # 写入事先创建好的txt文件中
    fp.write(title_list + ':' + content + '\n')
    print(title_list + '爬取成功')
fp.close()
```



### xpath（最常用）

最常用，最高效的一个数据解析方式

**数据解析原理：**

- 实例化etree对象，并且需要被解析的页面源码数据加载到该对象中

  `from lxml import etree`

  - 将本地的html文档加载进去

    `etree.parse(filePath)`

  - 从互联网上获取的页面源码加载进去

    `etree.HTML('page_text')`

    `page_text = requests.get(url=url, headers=headers).text`

- 标签定位和内容的捕获，调用etree对象中的xpath方法结合xpath表达式实现

  `xpath('xpath表达式')`

  只需要**xpath表达式**就可以获取数据了

**xpath表达式：**

==可以在检查页面右键点击Copy==

```html
<html lang="len">
    <head>
        <title>测试xpath</title>
    </head>
</html>
```

想要获取**title标签**中的内容

**属性定位：**`标签名[@属性名=""]`
**获取属性：**`标签名/@属性名`   

```python
from lxml import etree
tree_text = etree.parse('./test.html')
tree_text.xpath('/html/head/title')   # /html 表示从根节点开始遍历搜索，也就是从一开始开始
tree_text.xpath('/html//title')   # // 表示中间还有其他的，但是忽略，只需要定位到title就行，表示多个层级，并且可以从任意位置开始 //title 也行
# 此时返回的是一个 Element的对象，保存的是内存地址，并不能看出其中的内容


# 实现属性定位
tree_text.xpath('//div[@class="class选择器名"]')   # 返回的都是列表，并且都是Element

# 属性进行索引定位，从index=1开始
tree_text.xpath('//div[@class="class选择器名"]/p[3]')    # p是标签名，并且获取到了第三个p标签的数据，如果不加的话就是所有的p标签

```

**获取文本：/text()**

`tree_text.xpath('//div[@class="class选择器名"]//li[5]/a/text()')`
拿到  <li> 标签，是第五个 li 标签下的 标签a 的文本数据， 也是存入一个**列表**之中的

**获取贵阳二手房源信息：**

```python
from lxml import etree
import requests

# 需要UA伪装
# 将对应的User-Agent封装到一个字典当中
# 这个是在Network中检查的，在页面响应的瞬间可以查看，查看了一下，百度和搜狗的这一项是一样的
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66'
}

url = 'https://gy.58.com/ershoufang'
page_text = requests.get(url=url, headers=headers).text
tree_text = etree.HTML(page_text)
# a = tree_text.xpath('//div[@class="list-info"]/h2[@class="title"]/a/text()')
a = tree_text.xpath('//ul[@class="house-list-wrap"]/li')
for li in a:
    # 获取其标签值
    title = li.xpath('./div[2]/h2/a/text()')[0]
    print(title)
```

**获取图片：**

```python
from lxml import etree
import requests
import os

# 需要UA伪装
# 将对应的User-Agent封装到一个字典当中
# 这个是在Network中检查的，在页面响应的瞬间可以查看，查看了一下，百度和搜狗的这一项是一样的
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66'
}

url = 'http://pic.netmian.com/4kmeinv/'
response = requests.get(url=url, headers=headers)
# 设置响应的数据为’utf-8‘的编码格式，有一些是没法手动给赋值然后更改的
response.encoding = 'utf-8'
# response.encoding = response.apparent_encoding   # 这个也是可以的
page_text = response.text
# 数据解析
page_tree = etree.HTML(page_text)
li_list = page_tree.xpath('//div[@class="slist"]/ul/li')

# 创建文件夹
if not os.path.exists('./meinv'):
    os.mkdir('./meinv')

for li in li_list:
    img_src = li.xpath('./a/img/@src')[0]
    img_url = 'http://pic.netmian.com' + img_src
    img_name = li.xpath('./a/img/@alt')[0] + '.jpg'
    # img_name发生中文乱码，可以在这里设定乱码设定
    # 通用处理解决中文乱码
    img_name = img_name.encode('iso-8859-1').decode('gbk')
    # 数据持久化
    img_data = requests.get(url=img_url, headers=headers).content
    img_path = './meinv' + img_name
    with open(img_path, 'wb') as fp:
        fp.write(img_data)
        print(img_name + ' 下载成功！')
```

# 登录

**验证是否请求数据响应成功：**

```python
response = requests.post(url=, headers=, data=)
print(response.status_code)
# 返回200表示响应数据成功
```



**验证码识别：**使用别人的软件，获取需要识别的验证码的图片url，然后编写一定的代码来实现验证码数据的识别

或者使用**tesserocr**库，是**OCR识别库**，这个是需要安装**tesserocr库**的，并且需要下载好windows版本的**tesserocr.exe**，要绑定起来



点击登录按钮之后，会发送**POST**请求，所以会携带各种参数



但是在发起第二次请求的时候，服务器端是不确定这边是在登录状态下发起的请求，所以会有页面数据不会被捕捉

**cookie：**使服务器端保留当前客户端的相关状态

- 手动处理：通过抓包工具获取**Cookie值**，将其封装到**headers**中，不建议使用

  ```python
  import request
  headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66',
      'Cookie': 'SINAGLOBAL=6159395452423.373.1608125261481; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WhAhdyRXeSuL.UGsXmOVaW05JpX5KMhUgL.Fo-X1hnpeo5XSK-2dJLoIpW_9gUVqPxLi--Xi-zRi-zc; ALF=1641296724; SSOLoginState=1609760725; SCF=AjypbPnTr8IS2yXir07oljjbEf04Wwc-9_vYgcpY81NMdodGCz2-tEy4Ty5COmGoljKngwr0Xi9r3eNIR9qhzk4.; SUB=_2A25y9o-FDeRhGeNK41oQ8i7IzjmIHXVRheZNrDV8PUNbmtB-LXLikW9NSQdXfWGu6KByO2gBEMQcCRRuYQQqwrmq; wvr=6; wb_view_log_5488120455=1920*10801; _s_tentry=login.sina.com.cn; UOR=,,www.baidu.com; Apache=7290554933920.386.1609760734826; ULV=1609760734948:2:1:1:7290554933920.386.1609760734826:1608125261488; webim_unReadCount=%7B%22time%22%3A1609762051669%2C%22dm_pub_total%22%3A0%2C%22chat_group_client%22%3A0%2C%22chat_group_notice%22%3A0%2C%22allcountNum%22%3A13%2C%22msgbox%22%3A0%7D'
  }
  
  url = 'https://weibo.com/u/5488120455/home'
  response = requests.get(url=url, headers=headers)
  # 设置响应的数据为’utf-8‘的编码格式，有一些是没法手动给赋值然后更改的
  page_text = response.text
  with open('./text.html', 'w', encoding='utf-8') as fp:
      fp.write(page_text)
  ```

  

- 自动处理：

  **Cookie**来源，在登录界面中，使用抓包工具，可以获取到**Set-Cookie**这个值

  所以可以在**模拟post请求**登录的时候，获取由服务器端创建的**Cookie**

  - **session**会话对象：

    可以请求发送

    如果请求过程中产生了**Cookie**，那么该过程中的**Cookie**会被**自动存储/携带**到session对象中

**过程：**

- 创建一个**session对象**

  `session = requests.Session()`

- 使用**session对象**模拟登陆的**post请求**的发送（此时创建的**Cookie**会被存储在**session**对象之中）

  使用**Session对象**来获取请求

  `response = session.post(url=post_url, data=data, headers=headers)`

- **session对象**对主页对应的**get请求**的发送（携带了**Cookie**）

  `page_text = session.get(url=, headers=headers)`

- ==此时服务器端知道是在登录状态下的请求==

此时不需要在**headers**中封装**Cookie值**

# 代理

多次使用同一个**ip**，请求所对应的**ip**被服务器端封了，所以需要伪装**ip**，此时就出现了**代理**
	可以突破**ip**的访问限制，可以隐藏自身的**ip**

## 代理相关网站

- 快代理
- 西祠代理
- www.goubanjia.com

## 代理ip的类型

- **http：**应用到**http协议**对应的**url**之中
- **https：**应用到**https协议**对应的**url**之中

```python
import requests

headers={}
url = 'https://www.baidu.com/s?wd=ip'
page_text = requests.get(url=url, headers=headers, proxies={"https": '60.167.20.2:8888'}).text
with open('./ip.html', 'w', encoding='utf-8') as fp:
    fp.write(page_text)
```

==免费的ip代理不好找了==

# 异步爬虫

使用**异步**来实现**高性能**的数据爬取操作

```python
url = [
    '',
    '',
    ''
]

# 用于获取图片数据
def get_content(url):
    print('正在爬取：' + url)
    # get方法是一个阻塞的方法，一旦url为多个，那么就无法使用多线程工作，降低了效率
    response = requests.get(url=url, headers=headers)
    if response.status_code == 200:
        return response.content

# 用于数据解析
def parse_content(content):
    pass
```

- 线程池、进程池（适度使用：具备上限）

## 线程池

- 导入线程池模块对应的类

  ```python
  from multiprocessing.dummy import Pool
  ```

- 实例化一个线程池对象

  ```python
  pool = Pool(4)
  # 在线程池中开辟了4个线程
  ```

- 使用线程池

  ```python
  url_list = [
      '',
      '',
      '',
      ''
  ]
  
  pool.map(self, func, iterable)
  # 这是这个函数需要传入的参数
  pool = Pool(4)
  pool.map(get_text, url_list)
  # 将列表中的每一个元素都交给第一个参数（也就是方法）中使用，这里使用方法名，不调用该方法
  ```


### 下载网页中的视频

- 通过点击播放按钮找到对应的**.mp4**文件

  ![image-20210105094441110](C:\Users\梅桂楠\AppData\Roaming\Typora\typora-user-images\image-20210105094441110.png)

```python
url = 'https://video.pearvideo.com/mp4/third/20201229/cont-1714047-15690592-211425-ld.mp4'
response = requests.get(url=url, headers=headers)
page_text = response.content
with open('./text.mp4', 'wb') as fp:
    fp.write(page_text)
# 对于.mp4文件来说，使用的就是content和wb的写法
```

- 通过在页面加载的时候获取**XHR**中的字典，字典当中存储着**url地址**

```python
url = 'https://www.pearvideo.com/videoStatus.jsp?contId=1714047&mrd=0.17812027434539623'
response = requests.get(url=url, headers=headers)
page_text = response.text
with open('./text.json', 'w', encoding='utf-8') as fp:
    json.dump(page_text, fp, ensure_ascii=False)
```

通过这个获取的有些网站会进行加密，无法获取其正确的**字典**

## 异步

现在越来越多的框架都在流行**异步**

### 协程

不是计算机提供，是程序员创造的（微线程），是一种用户态内的上下文切换技术，通过一个线程来实现代码块的相互切换执行

**实现：**

- 第三方模块：**greenlet**，早期的模块
- **yield**关键字，可以保存状态，然后切换到其他代码去执行再切换回来
- **asyncio**模块，通过其装饰器再配合**yield**关键字来实现
- **async、await**关键字，目前最多

#### greenlet

```
pip install greenlet
一般来说下载了Anaconda了，就不需要再安装这个模块了
```

```python
from greenlet import greenlet


def func1():
    print(1)        # 2
    gr2.switch()    # 3，切换到 func2 函数去执行
    print(2)        # 6，因为 func2 中切换回来了，并且可以从上一次执行的地方接着执行
    gr1.switch()    # 7

def func2():
    print(3)        # 4
    gr1.switch()    # 5
    print(4)        # 8

gr1 = greenlet(func1)
gr2 = greenlet(func2)
gr1.switch()   # 第一步，去执行 func1 函数
```

#### yield关键字

```python
# 函数中存在 yield关键字，那么就是一个生成器函数
def func1():
    yield 1
    yield from func2()
    yield 2
def func2():
	yield 3
    yield 4
# 执行了一个生成器函数，返回了一个生成器为f1
f1 = func1()
for item in f1:
	print(item)
```

使用这种方法来写协程没有意义

#### asyncio

```python
import asyncio

@asyncio.coroutine   # 表示使用这个模块中的装饰器，给这个函数装饰一下，让普通函数转变成coroutine函数，也就是协程函数
def func1():
    print(1)
    yield from asyncio.sleep(2)       # 通过IO耗时操作，自动切换到tasks中的其他任务
    print(2)

@asyncio.coroutine
def func2():
    print(3)
    yield from asyncio.sleep(2)   # 在阻塞的过程中，会去执行别的函数
    print(4)

tasks = [
    asyncio.ensure_future(func1()),
    asyncio.ensure_future(func2())
]
# 封装成了一个列表，表示在python中会同时执行这两个协程函数

loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(tasks))
```

**协程函数**（被协程装饰器装饰的函数）**运行**：

```python
loop = asyncio.get_event_loop
loop.run_until_complete(函数)
# 只有这样，才能执行该函数
```

==遇到IO阻塞自动切换==

所以可以在网络请求下载的时候，可以同步执行多个，在下载等待的时候可以执行别的

#### async & await 关键字

```python
import asyncio

async def func1():
    print(1)
    await asyncio.sleep(2)       # 通过IO耗时操作，自动切换到tasks中的其他任务
    print(2)

async def func2():
    print(3)
    await asyncio.sleep(2)   # 在阻塞的过程中，会去执行别的函数
    print(4)

tasks = [
    asyncio.ensure_future(func1()),
    asyncio.ensure_future(func2())
]
# 封装成了一个列表，表示在python中会同时执行这两个协程函数

loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(tasks))
```

一般来说，就是不需要**装饰器**和**yield from**了

### 事件循环

可以理解成一个死循环，去检测并执行某些代码

```
# 伪代码

任务列表 = [任务1，任务2，任务3，。。。]

while True：
	可执行任务列表，已完成任务列表 = 去任务列表中检查所有任务，将“可执行”和“已完成”返回给前面两个列表
	
	for 就绪任务 in 可执行任务列表：
		执行已就绪任务
	
	for 已完成任务 in 已完成任务列表：
		在任务列表中移除 已完成任务
	
	如果任务列表中的任务都已完成，则终止循环
```

```python
import asyncio

# 去生成或获取一个事件循环
loop = asyncio.get_event_loop()

# 将任务放到“任务列表”
loop.run_until_complete(任务)
```

#### 实现

**协程函数：**定义函数时，加上了`async def 函数名`。或者是使用了装饰器`@asyncio.coroutine`，运行必须搭配**事件循环**

**协程对象：**执行协程函数得到的一个对象

协程对象 = 协程函数（）

```python
# 定义一个协程函数
async def func():
    pass

# 获取一个协程对象
result = func()
```

==执行协程函数创建协程对象的时候，函数内部的代码不会被执行==

搭配**事件循环**来执行

```python
import asyncio

# 定义一个协程函数
async def func():
    pass

# 获取一个协程对象
result = func()

# 执行函数内部的代码，交给事件循环来执行
loop = asyncio.get_event_loop()
loop.run_until_complete(result)

# python 3.7 更简洁的写法
# asyncio.run(result)
```

### await关键字

await + 可等待对象（协程对象，Future对象，Task对象 -> IO等待）

```python
import asyncio

async def func():
	print(1)
    response = await asyncio.sleep(2)  # 此时的resposne是None，但是对于其他的可以获取其请求

# 两种事件循环实现方式
loop = asyncio.get_event_loop()
loop.run_until_complete(func())

asyncio.run(func())  # python3.7
```

```python
import asyncio

async def others():
	print('start')
    await asyncio.sleep(2)
    print('end')
    return '返回值'

async def func():
	print('执行协程函数内部代码')
	
    # 遇到IO阻塞操作，挂起当前协程（任务），等待IO操作之后再继续完成，在这期间，事件循环会执行其他协程（任务）
    response = await others()
    
    print('IO请求结束，结果为：', response)

asyncio.run(func())
```

实例：

当 **await** 不执行完毕，后续代码不会被执行，且在一个代码块中可以存在多个 **await关键字**

```python
import asyncio

async def others():
	print('start')
    await asyncio.sleep(2)
    print('end')
    return '返回值'

async def func():
	print('执行协程函数内部代码')
	
    # 遇到IO阻塞操作，挂起当前协程（任务），等待IO操作之后再继续完成，在这期间，事件循环会执行其他协程（任务）
    response1 = await others()
    print('IO请求结束，结果为：', response1)
    
    response2 = await others()
    print('IO请求结束，结果为：', response2)

asyncio.run(func())
```

### Task对象

在事件循环过程中，可以并发地添加多个任务，哎任务列表中，实现真正的并发执行

创建**Task**对象：`asyncio.create_task(协程对象)`，可以让协程加入事件循环中等待调度的执行

```python
import asyncio


async def func():
    print(1)
    await asyncio.sleep(2)
    print(2)
    return '返回值'

async def main():
    print('main开始')

    # 创建一个Task对象，将任务添加到事件循环中，也就是func（）
    task1 = asyncio.create_task(func())
    task2 = asyncio.create_task(func())

    print('main结束')

    # 当执行某协程的时候，遇到IO阻塞，会自动执行其他任务，此处的await是等待相应的协程全部执行完毕并获取结果
    ret1 = await task1
    ret2 = await task2
    print(ret1, ret2)


asyncio.run(main())
```

以上的代码很少见，只是实例

**常见代码：**

```python
import asyncio


async def func():
    print(1)
    await asyncio.sleep(2)
    print(2)
    return '返回值'

async def main():
    print('main开始')

    # 创建一个Task对象，将任务添加到事件循环中，也就是func（）,也可以给task添加名字
    task_list = [
        asyncio.create_task(func(), name='n1'),
        asyncio.create_task(func(), name='n2')
    ]

    print('main结束')

    # 返回done和pending，返回值都会放到done这个集合对象之中
    # pending无意义，只有当存在未完成任务的时候，才会存入pending中
    # 可以增加一个参数 timeout=None 表示无时间限制，有的话就会存在任务未完成情况
    done,pending = await asyncio.wait(task_list)
    print(done)


asyncio.run(main())
```

当**task_list**写在外边的时候，必须要在事件循环创建之前，所以代码需要改动：

```python
task_list = [
    func(),
    func(),
]
done,pending = await asyncio.run( asyncio.wait(task_list) )
print(done)
```

==最好还是写里面==

### Future对象

是Task类的基类，Task继承Future，Task对象内部**await**结果处理就是基于Future对象

```python
async def main():
	# 获取当前事件循环
    loop = 	asyncio.get_running_loop()
    
    # 创建一个任务（Future对象），这个任务什么也不干
    ful = loop.create_future()
    
    # 等待任务最终结果，没有结果则会一直等待下去
    await ful
asyncio.run( main() )
```

只有当未来给这个**future**对象赋了值，才会拿到结果，这个任务才会结束

```python
import asyncio


async def set_after(fut):
    await asyncio.sleep(2)
    fut.set_result('666')

async def main():
    # 获取当前事件循环
    loop = asyncio.get_running_loop()

    # 创建一个Future对象
    fut = loop.create_future()

    # 创建一个任务（Task对象），绑定了set_after函数，函数内部在2秒之后会给fut赋值
    # 手动给fut赋值，让Future对象任务可以结束
    await loop.create_task(set_after(fut))

    # 等待Future对象获取结果，不然任务不会结束
    data = await fut
    print(data)

asyncio.run(main())
```

==Future对象一般不会手动赋值==

### concurrent.futures.Future对象

使用线程池、进程池实现异步操作时用到的对象

```python
import time
from concurrent.futures import Future
from concurrent.futures.thread import ThreadPoolExecutor
from concurrent.futures.process import ProcessPoolExecutor


def func(value):
    time.sleep(1)
    print(value)


# 创建线程池，最多具有5个线程
pool = ThreadPoolExecutor(max_workers=5)
# 创建进程池
# pool = ProcessPoolExecutor(max_workers=5)

for i in range(10):
    # submit表示：拿一个线程/进程来执行这个函数，i是传入这个函数的参数
    fut = pool.submit(func, i)
    print(fut)
```

在写代码的过程中，可能会存在交叉时间，就是一般来说要么使用协程，要么使用进程池或线程池，但是可以交叉使用

**例如：**crm项目（80%都是基于协程异步编程 + 第三方模块（不支持的话）【线程、进程做异步编程】）

默认会创建一个线程池

```python
import time
import asyncio
import concurrent.futures


def func1():
    time.sleep(2)
    return 'sb'


async def main():
    loop = asyncio.get_running_loop()

    # 1. Run in the default loop's executor(默认为ThreadPoolExecutor)
    # 第一步：内部会先调用 ThreadPoolExecutor 的 submit 方法去线程池中申请一个线程来执行 func1函数，并返回一个concurrent.futures.Future对象
    # 第二部，调用 asyncio.wrap_future 将 concurrent.futures.Future对象包装成一个 asyncio.Future对象
    # 因为 concurrent.futures.Future对象不支持 await 语法，所以需要包装才能使用
    fut = loop.run_in_executor(None, func1)
    result = await fut
    print('default thread pool', result)

    # 2. Run in a custom thread pool
    # with concurrent.futures.ThreadPoolExecutor() as pool:
    #     result = await loop.run_in_executor(pool, func1)
    #     print('custom thread pool', result)
    
    # 3. Run in custom process pool
    # with concurrent.futures.ProcessPoolExecutor() as pool:
    #     result = await  loop.run_in_executor(pool, func1)
    #     print('custom process pool', result)


asyncio.run(main())
```

**案例：**asyncio + 不支持异步的模块

```python
import asyncio
import requests


async def download_image(url):
    # 发送网络请求，下载图片（遇到网络下载图片的IO请求，自动切换到其他任务）
    print('开始下载：', url)

    loop = asyncio.get_event_loop()
    # request 模块默认不支持异步操作，所以只能使用线程池来实现异步
    future = loop.run_in_executor(None, requests.get, url)

    response = await future
    print('下载完成')
    # 将图片保存到本地文件
    file_name = url.rsplit('_')[-1]
    with open(file_name, mode='wb') as file_object:
        file_object.write(response.content)


if __name__ == '__main__':
    url_list = [
        '',
        '',
        ''
    ]
    tasks = [
        download_image(url) for url in url_list
    ]
    loop = asyncio.get_event_loop()
    loop.run_until_complete(asyncio.wait(tasks))
```

==这个效果是一样的，但是耗费的资源更多==

### 异步迭代器

迭代器：内部可以使用**iter方法**和**next方法**

**异步迭代器：**

实现了`__aiter__()`和`__anext__()`方法的对象，`__anext__`必须返回一个 `awaitable`对象。`async for `会处理异步迭代器的`__anext__()`方法所返回的可等待对象，直到其引发一个`StopAsyncIetration`异常

**异步可迭代对象：**

可在`async for`语句中使用的对象，必须通过它的`__anext__()`方法返回一个`asynochronous iterator`

**实例：**

```python
import asyncio


class Reader(object):
    ''' 自定义异步迭代器，也是异步可迭代器对象 '''

    def __init__(self):
        self.count = 0

    async def readline(self):
        # 必须得使用async，不然实现函数的对象就不是协程对象
        await asyncio.sleep(1)
        self.count += 1
        if self.count == 100:
            return None
        return self.count

    def __aiter__(self):
        # 迭代器返回的是自己
        return self

    async def __anext__(self):
        # anext是一个一个往下请求的，所以必须得加async
        val = await self.readline()
        if val == None:
            # 触发异常，不能再往下继续请求了
            raise StopAsyncIteration
        return val


async def func():
    # 迭代器对象可以被for循环，但是比较特殊
    obj = Reader()
    # async for语句比较特殊，必须写在协程函数内，不在函数内部就会报错
    async for item in obj:
        print(item)


asyncio.run(func())
```

### 异步上下文管理器

==用处很大==

此对象通过定义`__aenter__()`和`__aexit__()`方法来对`async with`语句中的环境进行控制

```python
import asyncio


class AsyncContextManager:
    def __init__(self):
        self.conn = conn

    async def do_something(self):
        # 异步操作数据据库
        return 666

    # 先执行这个函数
    async def __aenter__(self):
        # 异步链接数据库
        self.conn = await asyncio.sleep(1)
        return self
        # return的就是f

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        # 异步关闭数据库连接
        await asyncio.sleep(1)


async def func():
    # async with 也必须在协程函数内部
    # obj = AsyncContextManager()
    # async with obj:
    #     pass
    async with AsyncContextManager() as f:
        result = await f.do_something()
        print(result)

asyncio.run(func())
```

只要涉及到**打开、关闭**的操作，内部一般都会定义有这个

# uvloop

是 **asyncio 事件循环**的替代方案，在一定程度上可以提高事件循环的**效率**

`pip3 install uvloop`

```python
import asyncio
# 只需要以下这两句代码，就可以替换成功，其余代码与之前是一样的
import uvloop
asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())

# 编写asyncio代码，与之前写的代码一致

# 内部事件循环自动化会编程uvloop
asyncio.run()
```



**asgi：**内部存在一个`uvicorn`，都是用了它所以才快，**Django3**也是因为这个，其内部默认使用的就是`uvloop`

# 异步实战

首先需要思考当中是否有支持异步操作的相关模块，如果具备的话就可以按照模板来进行相关的操作

## 异步操作redis

一般来说，就是遇到IO的时候，就加上`await`，让其可以执行别的任务

在使用python代码操作redis时，链接/操作/断开都是**网络IO**

想实现异步，就必须安装这个模块**aioredis**

```
pip3 install aioredis
```

示例1：

```python
import asyncio
import aioredis


async def execute(address, password):
    print('开始执行', address)
    # 网络IO操作，创建redis链接
    redis = await aioredis.create_redis(address, password=password)

    # 网络IO操作，在redis中设置哈希值car，内部再设置三个键值对
    # redis = {car: {key1: 1, key2: 2, key3: 3}}
    await redis.hmset_dict('car', key1=1, key2=2, key3=3)

    # 网络IO操作，去redis中获取值
    result = await redis.hgetall('car', encoding='utf-8')
    print(result)

    redis.close()
    # 网络IO操作，关闭redis链接
    await redis.wait_closed()

    print('结束',address)


asyncio.run(execute('redis://47.93.4.198.6379', 'root!2345'))
```

实例2：

```python
import asyncio
import aioredis


async def execute(address, password):
    print('开始执行', address)

    # 网络IO操作，创建redis链接
    # 先去链接第一个任务，然后遇到IO切换任务，去链接下一个任务
    redis = await aioredis.create_redis(address, password=password)

    # 网络IO操作，在redis中设置哈希值car，内部再设置三个键值对
    # redis = {car: {key1: 1, key2: 2, key3: 3}}
    await redis.hmset_dict('car', key1=1, key2=2, key3=3)

    # 网络IO操作，去redis中获取值
    result = await redis.hgetall('car', encoding='utf-8')
    print(result)

    redis.close()
    # 网络IO操作，关闭redis链接
    await redis.wait_closed()

    print('结束',address)


task_list = [
    execute('redis://47.93.4.198.6379', 'root!2345'),
    execute('redis://47.93.4.197.6379', 'root!2345')
]
asyncio.run(asyncio.wait(task_list))
```

## 异步MySQL

```python
pip3 install aiomysql
```

实例1：

```python
import asyncio
import aiomysql


async def execute():
    # 网络IO操作，链接MySQL
    coon = await aiomysql.connect(host='127.0.0.1', port=3306, ueer='root', password='123', db='mysql')

    # 网络IO操作，创建CURSOR
    cur = await coon.cursor()

    # 网络IO操作，执行SQL
    await cur.execute('SELECT Host,User FROM user')

    # 网络IO操作，获取SQL结果
    result = await cur.fetchall()
    print(result)

    # 网络IO操作，关闭SQL
    await cur.close()
    coon.close()
    print('结束', host)

asyncio.run(execute())
```

实例2：

```python
import asyncio
import aiomysql


async def execute(host, password):
    # 网络IO操作，链接MySQL
    coon = await aiomysql.connect(host=host, port=3306, ueer='root', password=password, db='mysql')

    # 网络IO操作，创建CURSOR
    cur = await coon.cursor()

    # 网络IO操作，执行SQL
    await cur.execute('SELECT Host,User FROM user')

    # 网络IO操作，获取SQL结果
    result = await cur.fetchall()
    print(result)

    # 网络IO操作，关闭SQL
    await cur.close()
    coon.close()
    print('结束', host)
    
task_list = [
    execute('47.93.4.198.6379', 'root!2345'),
    execute('47.93.4.198.6379', 'root!2345')
]

asyncio.run(asyncio.wait(task_list))
```

## FastAPI框架

只是以这个框架来为例子，其余的框架也是一样的

```
pip3 install fastapi
pip3 install uvicorn(asgi内部支持uvloop)
```

```python
import asyncio
from fastapi import FASTAPI
import uvicorn

app = FASTAPI()

# 创建一个redis连接池
REDIS_POOL = aioredis.ConnectionsPool('redis://47.193.14.198:6379', password='root123', insize=1, maxsize=10)

@app.get('/')
def index():
    '''普通操作接口'''
    return {'message': 'hello word'}


@app.get('/red')
async def red():
    '''异步操作接口'''
    print('请求来了')
    await asyncio.sleep(3)
    # 连接池获取一个连接
    conn = await REDIS_POOL.acquire()
    redis = Redis(conn)

    # 设置值
    await redis.hmset_dict('car', key1=1, key2=2, key3=3)

    # 读取值
    result = await redis.hgetall('car', encoding='utf-8')
    print(result)

    # 连接归还连接池
    REDIS_POOL.release(conn)

    return result

if __name__ == '__main__':
    uvicorn.run('luffy:app', host='127.0.0.1', port=5000, log_level='info')
# luffy：是这个python脚本的名称，app是指代码内部的app对象
```

## 爬虫

```python
pip3 install aiohttp
```

### 发送请求

```python
async def fetch():
    async with aiohttp.ClientSession() as session:
        async with session.get('https://www.baidu.com') as resposne:
            print(await resposne.text())
loop = asyncio.get_event_loop()
tasks = [fetch(),]
loop.run_until_complete(asyncio.wait(tasks))
```

### 添加请求参数

```python
params = {'key': 'value', 'page': 10}
async def fetch():
    async with aiohttp.ClientSession() as session:
        async with session.get('https://www.baidu.com/s',params=params) as resposne:
            print(await resposne.url)
loop = asyncio.get_event_loop()
tasks = [fetch(),]
loop.run_until_complete(asyncio.wait(tasks))
```

### UA伪装

```python
url = 'http://httpbin.org/user-agent'
headers = {'User-Agent': 'test_user_agent'}
async def fetch():
    async with aiohttp.ClientSession() as session:
        async with session.get(url,headers=headers) as resposne:
            print(await resposne.text())
loop = asyncio.get_event_loop()
tasks = [fetch(),]
loop.run_until_complete(asyncio.wait(tasks))
```

### 自定义Cookie

```python
url = 'http://httpbin.org/cookies'
cookies = {'cookies_name': 'test_cookies'}
async def fetch():
    async with aiohttp.ClientSession() as session:
        async with session.get(url,cookies=cookies) as resposne:
            print(await resposne.text())
loop = asyncio.get_event_loop()
tasks = [fetch(),]
loop.run_until_complete(asyncio.wait(tasks))
```

### post请求参数

```python
url = 'http://httpbin.org'
payload = {'username': 'zhang', 'password': '123456'}
async def fetch():
    async with aiohttp.ClientSession() as session:
        async with session.post(url, data=payload) as resposne:
            print(await resposne.text())
loop = asyncio.get_event_loop()
tasks = [fetch(), ]
loop.run_until_complete(asyncio.wait(tasks))
```

### 设置代理

```python
url = "http://python.org"
async def fetch():
    async with aiohttp.ClientSession() as session:
        async with session.get(url, proxy="http://some.proxy.com") as resposne:
        print(resposne.status)
loop = asyncio.get_event_loop()
tasks = [fetch(), ]
loop.run_until_complete(asyncio.wait(tasks))
```

### 异步IO处理

```python
#环境安装：pip install aiohttp
#使用该模块中的ClientSession
import requests
import asyncio
import time
import aiohttp
start = time.time()
urls = [
    'http://127.0.0.1:5000/bobo','http://127.0.0.1:5000/jay','http://127.0.0.1:5000/tom',
    'http://127.0.0.1:5000/bobo', 'http://127.0.0.1:5000/jay', 'http://127.0.0.1:5000/tom',
    'http://127.0.0.1:5000/bobo', 'http://127.0.0.1:5000/jay', 'http://127.0.0.1:5000/tom',
    'http://127.0.0.1:5000/bobo', 'http://127.0.0.1:5000/jay', 'http://127.0.0.1:5000/tom',
]
async def get_page(url):
    async with aiohttp.ClientSession() as session:
        #get()、post():
        #headers,params/data,proxy='http://ip:port'
        async with await session.get(url) as response:
            #text()返回字符串形式的响应数据
            #read()返回的二进制形式的响应数据
            #json()返回的就是json对象
            #注意：获取响应数据操作之前一定要使用await进行手动挂起
            page_text = await response.text()
            print(page_text)
tasks = []
for url in urls:
    c = get_page(url)
    task = asyncio.ensure_future(c)
    tasks.append(task)
loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(tasks))
end = time.time()
print('总耗时:',end-start)
```

### 案例

```python
import asyncio
import aiohttp


async def fetch(session, url):
    print('发送请求：', url)
    async with session.get(url, verify_ssl=False) as response:
        text = await response.text()
        print('得到结果：', url, len(text))


async def main():
    async with aiohttp.ClientSession() as session:
        url_list = [
            'https://www.baidu.com',
            'heeps://python.org'
        ]
        tasks = [asyncio.create_task(fetch(session, url)) for url in url_list]

        done, pending = await asyncio.wait(tasks)

if __name__ == '__main__':
    asyncio.run(main())
```

































