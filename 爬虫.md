# 案例

代码没有分行显示，可以  ctrl+alt+L 来实现分行处理

## 爬取对应词条的搜索页面

```python
import requests

# 需要UA伪装
# 将对应的User-Agent封装到一个字典当中
# 这个是在Network中检查的，在页面响应的瞬间可以查看
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66'
}
kw = str(input('enter:'))
url = 'https://www.sogou.com/web?query={0}'.format(kw)
response = requests.get(url=url, headers=headers)
page_txt = response.text
with open('./save.html', 'w', encoding='utf-8') as fp:
    fp.write(page_txt)
    # 对于爬取出来之后的代码没有分行显示，可以  ctrl+alt+l 来实现分行处理
    print('end')
```

`requests.get(url,param,headers)`三个参数，其中param可以对应的是**query**的值，也是可以保存在字典当中的

## 爬取翻译网站的文本框中的数据

一般来说定位到XHR中，可以找到该网站所请求的数据包

![image-20201229155100078](C:\Users\梅桂楠\AppData\Roaming\Typora\typora-user-images\image-20201229155100078.png)

明显是一个**POST**的请求，所以需要携带参数

![image-20201229155247743](C:\Users\梅桂楠\AppData\Roaming\Typora\typora-user-images\image-20201229155247743.png)

![image-20201229155430971](C:\Users\梅桂楠\AppData\Roaming\Typora\typora-user-images\image-20201229155430971.png)

**Content-Type：**表示服务端响应为客户端的数据类型，表示相应的数据是一组**JSON数据**

![image-20201229155542856](C:\Users\梅桂楠\AppData\Roaming\Typora\typora-user-images\image-20201229155542856.png)

就是服务器端响应回来的数据

```python
import requests
import json

# 需要UA伪装
# 将对应的User-Agent封装到一个字典当中
# 这个是在Network中检查的，在页面响应的瞬间可以查看，查看了一下，百度和搜狗的这一项是一样的
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66'
}
# 每个网页的url都不同
post_url = 'https://fanyi.baidu.com/sug'
# post请求参数处理，和get是一样的
word = input('enter the word:')
data = {
    'kw': word
}
# 获取请求，还是需要UA伪装
response = requests.post(url=post_url, data=data, headers=headers)
# 获取响应的数据：json()返回的是一个对象obj，检查发现是一个字典对象（如果确认响应数据类型是json类型才可使用）
dic_obj = response.json()
# 持久化存储
with open('./save.json', 'w', encoding='utf-8') as fp:
    # 不能使用ascii码
    json.dump(dic_obj, fp, ensure_ascii=False)
    # 对于爬取出来之后的代码没有分行显示，可以  ctrl+alt+l 来实现分行处理
    print('end')
```

这个相对应的在**有道翻译网页**上是行不通的

## 爬取肯德基位置信息

```python
import requests
import json

# 需要UA伪装
# 将对应的User-Agent封装到一个字典当中
# 这个是在Network中检查的，在页面响应的瞬间可以查看，查看了一下，百度和搜狗的这一项是一样的
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66'
}
# 每个网页的url都不同
cname = str(input('enter:'))
url = 'http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=cname'
data = {
    'cname': cname,
    'pageIndex': '1',   # 从第一个位置开始爬取
    'pageSize': '10'    # 只爬取十个
}
# post请求参数处理，和get是一样的
# 获取请求，还是需要UA伪装
response = requests.post(url=url, data=data, headers=headers)
# 获取响应的数据：json()返回的是一个对象obj，检查发现是一个字典对象（如果确认响应数据类型是json类型才可使用）
dic_obj = response.text
# 持久化存储
with open('./save.json', 'w', encoding='utf-8') as fp:
    # 不能使用ascii码
    json.dump(dic_obj, fp, ensure_ascii=False)
    # 对于爬取出来之后的代码没有分行显示，可以  ctrl+alt+l 来实现分行处理
    print('end')
```

## 爬取化妆品数据

这个主要在于还爬取了化妆品公司的详细信息，有一个多次跳动，也就是对**JSON**文件字段的提取

```python
import requests
import json

# 需要UA伪装
# 将对应的User-Agent封装到一个字典当中
# 这个是在Network中检查的，在页面响应的瞬间可以查看，查看了一下，百度和搜狗的这一项是一样的
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66'
}
# 每个网页的url都不同
url = 'http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do?method=getXkzsList'
data = {
    'on': 'true',
    'page': '1',
    'pageSize': '15',
    'productName': '',
    'conditionType': '1',
    'applyname': '',
    'applysn': ''
}
# post请求参数处理，和get是一样的
# 获取请求，还是需要UA伪装
response = requests.post(url=url, data=data, headers=headers)
# 获取响应的数据：json()返回的是一个对象obj，检查发现是一个字典对象（如果确认响应数据类型是json类型才可使用）
dic_obj = response.json()
# 持久化存储
with open('./save.json', 'w', encoding='utf-8') as fp:
    # 不能使用ascii码
    json.dump(dic_obj, fp, ensure_ascii=False)
    # 对于爬取出来之后的代码没有分行显示，可以  ctrl+alt+l 来实现分行处理
    print('end')

# 从json文件中获取ID的数据
id_list = []
all_data_list = []
for dic in dic_obj['list']:
    id_list.append(dic['ID'])

# 获取详情页的数据
urls = 'http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do?method=getXkzsById'
for ids in id_list:
    datas = {
        'id': ids
    }
    responses = requests.post(url=urls, data=datas, headers=headers)
    # 获取响应的数据：json()返回的是一个对象obj，检查发现是一个字典对象（如果确认响应数据类型是json类型才可使用）
    dic_objs = responses.json()
    all_data_list.append(dic_objs)
with open('./saves.json', 'w', encoding='utf-8') as fp:
    # 不能使用ascii码
    json.dump(all_data_list, fp, ensure_ascii=False)
    # 对于爬取出来之后的代码没有分行显示，可以  ctrl+alt+l 来实现分行处理
    print('end')
```

# 基础

.txt：返回字符串形式的数据

.json：返回对象类型的数据

.content：返回二进制格式的图片数据

## 聚焦爬虫

爬取页面中指定的页面内容

**爬虫流程**

- 指定URL
- 发起请求
- 获取响应数据
- 数据解析
- 持久化存储

## 数据解析

通过获取对应的**标签**来获取对应的数据，也可以从**标签**对应的**属性**中获取对应的数据

1 进行标签的定位

2 对标签或者标签对应的（属性）中存储的数据值进行存储（数据解析）

### 正则

![img](https://book.apeland.cn/media/images/2019/04/15/snip20190415_21.png)

#### 爬取糗事百科中糗图板块下的图片

一般来说先获取整张页面，然后使用聚焦爬虫，进行数据解析

![image-20201230210938226](C:\Users\梅桂楠\AppData\Roaming\Typora\typora-user-images\image-20201230210938226.png)

可以看出在前端标签的属性字段中存放着这个图片的**url地址：**<img src="">

所以可以在此获取所有图片的url，然后再批量的对这些url发送请求获取图片数据

可以看出：

```html
<div>
    <a>
    <img src=""/>
    </a>
</div>
```

所以首先需要找到所属的**div**标签，通常来说都是div标签套div标签

配置正则：`ex='<div class="thumb">.*?<img src="(.*?)" alt.*?</div>`

让正则作用到字符串当中：`re.findall(ex, img_text, re.S)`

**此时只能下载第一页的图片：**

```python
import requests
import os
import re

# 创建一个文件夹来保存所有爬取到的数据
if not os.path.exists('./img_test'):
    os.mkdir('./img_test')

# 需要UA伪装
# 将对应的User-Agent封装到一个字典当中
# 这个是在Network中检查的，在页面响应的瞬间可以查看，查看了一下，百度和搜狗的这一项是一样的
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66'
}
# 首先请求整个页面
url_text = 'https://www.qiushibaike.com/imgrank/'
# 使用通用爬虫对一整张页面进行爬取
img_text = requests.get(url=url_text, headers=headers).text
# 使用聚焦爬虫将页面中的请求进行数据解析
# 配置正则函数
ex = '<div class="thumb">.*?<img src="(.*?)" alt.*?</div>'
# 将正则作用于一个字符串中，re.S表示单行匹配，re.M表示多行匹配，返回的是一个列表
img_src_list = re.findall(ex, img_text, re.S)
for img_src in img_src_list:
    # 拼接出一个完整的图片的url地址
    img_url = 'https:' + img_src
    # content返回的是二进制形式的图片数据
    img = requests.get(url=img_url, headers=headers).content
    # 生成图片名称，从img_src根据‘/’来进行切割
    img_name = img_src.split('/')[-1]
    # 图片存储的路径
    img_path = './img_test/' + img_name
    # 存储的是二进制数据，所以是‘wb’
    with open(img_path, 'wb') as fp:
        fp.write(img)
        print(img_name + '下载成功！')
```

**这个可以获取不同页码的页面：**

```python
import requests
import os
import re

# 创建一个文件夹来保存所有爬取到的数据
if not os.path.exists('./img_test'):
    os.mkdir('./img_test')

# 需要UA伪装
# 将对应的User-Agent封装到一个字典当中
# 这个是在Network中检查的，在页面响应的瞬间可以查看，查看了一下，百度和搜狗的这一项是一样的
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66'
}
# 首先请求整个页面
# 想请求不同页码的数据，所以可以拼接以下不同页面的url
# url_all = []
# for page in range(1, 2):
    # url_all.append('https://www.qiushibaike.com/imgrank/page/{0}/'.format(page))
for page in range(1, 3):
    url_text = 'https://www.qiushibaike.com/imgrank/page/{0}/'.format(page)
    # 使用通用爬虫对一整张页面进行爬取
    img_text = requests.get(url=url_text, headers=headers).text
    # 使用聚焦爬虫将页面中的请求进行数据解析
    # 配置正则函数
    ex = '<div class="thumb">.*?<img src="(.*?)" alt.*?</div>'
    # 将正则作用于一个字符串中，re.S表示单行匹配，re.M表示多行匹配，返回的是一个列表
    img_src_list = re.findall(ex, img_text, re.S)
    for img_src in img_src_list:
        # 拼接出一个完整的图片的url地址
        img_url = 'https:' + img_src
        # content返回的是二进制形式的图片数据
        img = requests.get(url=img_url, headers=headers).content
        # 生成图片名称，从img_src根据‘/’来进行切割
        img_name = img_src.split('/')[-1]
        # 图片存储的路径
        img_path = './img_test/' + img_name
        # 存储的是二进制数据，所以是‘wb’
        with open(img_path, 'wb') as fp:
            fp.write(img)
            print(img_name + '下载成功！')
```

### bs4

**只能应用于python语言之中**

**数据解析原理：**

- 标签定位
- 提取标签、标签属性中存储的数据值

**bs4数据解析原理：**

- 实例化BeautifulSoup对象，并且将页面源码数据加载到该对象中
- 通过调用BeautifulSoup对象中的属性或者方法进行标签定位和数据提取

**实例化BeautifulSoup对象：**

- `from bs4 import BeautifulSoup`

- 对象的实例化

  - 将本地的html文档中的数据加载到该对象之中

    `BeautifulSoup（文件名，'lxml'）`

    ```python
    fp = open('./test.html', 'r', encoding='utf-8')
    soup = BeautifulSoup(fp, 'lxml')
    ```

  - 将互联网上获取的页面源码加载到该对象中

    ```python
    page_text = requests.get(url=url, headers=headers).text
    soup = BeautifulSoup(page_text, 'lxml')
    # 当然不一定是get，只需要获取到页面的源码就可以了
    ```

- 提供应用于数据解析的方法和属性

  - `soup.标签名`    只会得到整个源码中的**第一次**出现这个标签名字地方的数据

  - `soup.find('标签名')`返回的也是第一次出现这个标签名的地方

    ​	`soup.find('标签名', class_='属性名')`返回的就是**class属性**所对应的**标签**

  - `soup.find_all('标签名')`返回的是出现这个标签名的所有的地方，以**列表**的形式返回，当然这个也可以进行属性定位

  - `soup.select('选择器样式+选择器名')`比如**类选择器**就是`.class属性名`，**标签选择器什么都不加**，返回的是一个列表

    ```python
    soup.select('.tang > url > li > a')[0]
    # 可以使用这种方法来寻找层级关系的标签，返回的是列表形式，>表示下一个层级，[0]表示返回的是<a>标签中的第一个标签
    
    soup.select('.tang > url a')[0]
    # 空格表示多个层级，>表示一个层级
    ```

- 获取标签之间的文本数据

  ```python
  soup.select('.tang > url a')[0].text
  soup.select('.tang > url a')[0].string	
  soup.select('.tang > url a')[0].get_text()
  # 三个都是可以获取到该标签中的文本数据
  
  text/get_text()可以获取到标签当中所有的文本数据
  string只能获取到该标签中直系的文本内容
  # 获取直系的意思就是，如果这个标签下面没有文本，但是其下属标签中有文本，也不会被获取
  ```

- 获取标签中的属性值

  `soup.标签名['属性名称']`

  ```python
  soup.select('.tang > url a')[0]['href']
  # 获取到的就是href这个属性字段所对应的数据
  ```

**爬取三国演义小说中的所有的标题**

```html
这些标题都存储在以下的标签内
<body>
    <div id="main">
        <div id="main_left">
            <div class="card bookmark-list">
                <div class="book-mulu">
                    <ul>
                        <li>
                        	<a href="/book/sanguoyanyi/1.html">第一回·宴桃园豪杰三结义 斩黄巾英雄首立功</a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
```

```python
from bs4 import BeautifulSoup
import requests

# 需要UA伪装
# 将对应的User-Agent封装到一个字典当中
# 这个是在Network中检查的，在页面响应的瞬间可以查看，查看了一下，百度和搜狗的这一项是一样的
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66'
}

url = 'https://www.shicimingju.com/book/sanguoyanyi.html'
page_text = requests.get(url=url, headers=headers).text
soup = BeautifulSoup(page_text, 'lxml')
# # 获取每一章内容所对应的url
# page_contents = soup.select('.book-mulu > ul a')[0]['href']
# url_contents = 'https://www.shicimingju.com' + page_contents
# # 获取每一章的标题
# page_totalText = soup.select('.book-mulu > ul')[0].text

# 提前打开建立好一个文件，不然在for循环中都会重新建立新的文件
fp = open('./sanguo.txt', 'w', encoding='utf-8')

li_list = soup.select('.book-mulu > ul > li')
for li in li_list:
    title_list = li.a.text
    url_list = 'https://www.shicimingju.com' + li.a['href']
    page_content = requests.get(url=url_list, headers=headers).text
    # 解析出详情页中相关页的内容
    page_soup = BeautifulSoup(page_content, 'lxml')
    div_tag = page_soup.find('div', class_='chapter_content')
    # 获取解析到的章节中的内容
    content = div_tag.text
    # 写入事先创建好的txt文件中
    fp.write(title_list + ':' + content + '\n')
    print(title_list + '爬取成功')
fp.close()
```



### xpath（最常用）

最常用，最高效的一个数据解析方式

**数据解析原理：**

- 实例化etree对象，并且需要被解析的页面源码数据加载到该对象中

  `from lxml import etree`

  - 将本地的html文档加载进去

    `etree.parse(filePath)`

  - 从互联网上获取的页面源码加载进去

    `etree.HTML('page_text')`

    `page_text = requests.get(url=url, headers=headers).text`

- 标签定位和内容的捕获，调用etree对象中的xpath方法结合xpath表达式实现

  `xpath('xpath表达式')`

  只需要**xpath表达式**就可以获取数据了

**xpath表达式：**

==可以在检查页面右键点击Copy==

```html
<html lang="len">
    <head>
        <title>测试xpath</title>
    </head>
</html>
```

想要获取**title标签**中的内容

**属性定位：**`标签名[@属性名=""]`
**获取属性：**`标签名/@属性名`   

```python
from lxml import etree
tree_text = etree.parse('./test.html')
tree_text.xpath('/html/head/title')   # /html 表示从根节点开始遍历搜索，也就是从一开始开始
tree_text.xpath('/html//title')   # // 表示中间还有其他的，但是忽略，只需要定位到title就行，表示多个层级，并且可以从任意位置开始 //title 也行
# 此时返回的是一个 Element的对象，保存的是内存地址，并不能看出其中的内容


# 实现属性定位
tree_text.xpath('//div[@class="class选择器名"]')   # 返回的都是列表，并且都是Element

# 属性进行索引定位，从index=1开始
tree_text.xpath('//div[@class="class选择器名"]/p[3]')    # p是标签名，并且获取到了第三个p标签的数据，如果不加的话就是所有的p标签

```

**获取文本：/text()**

`tree_text.xpath('//div[@class="class选择器名"]//li[5]/a/text()')`
拿到  <li> 标签，是第五个 li 标签下的 标签a 的文本数据， 也是存入一个**列表**之中的

**获取贵阳二手房源信息：**

```python
from lxml import etree
import requests

# 需要UA伪装
# 将对应的User-Agent封装到一个字典当中
# 这个是在Network中检查的，在页面响应的瞬间可以查看，查看了一下，百度和搜狗的这一项是一样的
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66'
}

url = 'https://gy.58.com/ershoufang'
page_text = requests.get(url=url, headers=headers).text
tree_text = etree.HTML(page_text)
# a = tree_text.xpath('//div[@class="list-info"]/h2[@class="title"]/a/text()')
a = tree_text.xpath('//ul[@class="house-list-wrap"]/li')
for li in a:
    # 获取其标签值
    title = li.xpath('./div[2]/h2/a/text()')[0]
    print(title)
```

**获取图片：**

```python
from lxml import etree
import requests
import os

# 需要UA伪装
# 将对应的User-Agent封装到一个字典当中
# 这个是在Network中检查的，在页面响应的瞬间可以查看，查看了一下，百度和搜狗的这一项是一样的
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66'
}

url = 'http://pic.netmian.com/4kmeinv/'
response = requests.get(url=url, headers=headers)
# 设置响应的数据为’utf-8‘的编码格式，有一些是没法手动给赋值然后更改的
response.encoding = 'utf-8'
# response.encoding = response.apparent_encoding   # 这个也是可以的
page_text = response.text
# 数据解析
page_tree = etree.HTML(page_text)
li_list = page_tree.xpath('//div[@class="slist"]/ul/li')

# 创建文件夹
if not os.path.exists('./meinv'):
    os.mkdir('./meinv')

for li in li_list:
    img_src = li.xpath('./a/img/@src')[0]
    img_url = 'http://pic.netmian.com' + img_src
    img_name = li.xpath('./a/img/@alt')[0] + '.jpg'
    # img_name发生中文乱码，可以在这里设定乱码设定
    # 通用处理解决中文乱码
    img_name = img_name.encode('iso-8859-1').decode('gbk')
    # 数据持久化
    img_data = requests.get(url=img_url, headers=headers).content
    img_path = './meinv' + img_name
    with open(img_path, 'wb') as fp:
        fp.write(img_data)
        print(img_name + ' 下载成功！')
```

# 登录

**验证是否请求数据响应成功：**

```python
response = requests.post(url=, headers=, data=)
print(response.status_code)
# 返回200表示响应数据成功
```



**验证码识别：**使用别人的软件，获取需要识别的验证码的图片url，然后编写一定的代码来实现验证码数据的识别

或者使用**tesserocr**库，是**OCR识别库**，这个是需要安装**tesserocr库**的，并且需要下载好windows版本的**tesserocr.exe**，要绑定起来



点击登录按钮之后，会发送**POST**请求，所以会携带各种参数



但是在发起第二次请求的时候，服务器端是不确定这边是在登录状态下发起的请求，所以会有页面数据不会被捕捉

**cookie：**使服务器端保留当前客户端的相关状态

- 手动处理：通过抓包工具获取**Cookie值**，将其封装到**headers**中，不建议使用

  ```python
  import request
  headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66',
      'Cookie': 'SINAGLOBAL=6159395452423.373.1608125261481; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WhAhdyRXeSuL.UGsXmOVaW05JpX5KMhUgL.Fo-X1hnpeo5XSK-2dJLoIpW_9gUVqPxLi--Xi-zRi-zc; ALF=1641296724; SSOLoginState=1609760725; SCF=AjypbPnTr8IS2yXir07oljjbEf04Wwc-9_vYgcpY81NMdodGCz2-tEy4Ty5COmGoljKngwr0Xi9r3eNIR9qhzk4.; SUB=_2A25y9o-FDeRhGeNK41oQ8i7IzjmIHXVRheZNrDV8PUNbmtB-LXLikW9NSQdXfWGu6KByO2gBEMQcCRRuYQQqwrmq; wvr=6; wb_view_log_5488120455=1920*10801; _s_tentry=login.sina.com.cn; UOR=,,www.baidu.com; Apache=7290554933920.386.1609760734826; ULV=1609760734948:2:1:1:7290554933920.386.1609760734826:1608125261488; webim_unReadCount=%7B%22time%22%3A1609762051669%2C%22dm_pub_total%22%3A0%2C%22chat_group_client%22%3A0%2C%22chat_group_notice%22%3A0%2C%22allcountNum%22%3A13%2C%22msgbox%22%3A0%7D'
  }
  
  url = 'https://weibo.com/u/5488120455/home'
  response = requests.get(url=url, headers=headers)
  # 设置响应的数据为’utf-8‘的编码格式，有一些是没法手动给赋值然后更改的
  page_text = response.text
  with open('./text.html', 'w', encoding='utf-8') as fp:
      fp.write(page_text)
  ```

  

- 自动处理：

  **Cookie**来源，在登录界面中，使用抓包工具，可以获取到**Set-Cookie**这个值

  所以可以在**模拟post请求**登录的时候，获取由服务器端创建的**Cookie**

  - **session**会话对象：

    可以请求发送

    如果请求过程中产生了**Cookie**，那么该过程中的**Cookie**会被**自动存储/携带**到session对象中

**过程：**

- 创建一个**session对象**

  `session = requests.Session()`

- 使用**session对象**模拟登陆的**post请求**的发送（此时创建的**Cookie**会被存储在**session**对象之中）

  使用**Session对象**来获取请求

  `response = session.post(url=post_url, data=data, headers=headers)`

- **session对象**对主页对应的**get请求**的发送（携带了**Cookie**）

  `page_text = session.get(url=, headers=headers)`

- ==此时服务器端知道是在登录状态下的请求==

此时不需要在**headers**中封装**Cookie值**

# 代理

多次使用同一个**ip**，请求所对应的**ip**被服务器端封了，所以需要伪装**ip**，此时就出现了**代理**
	可以突破**ip**的访问限制，可以隐藏自身的**ip**

## 代理相关网站

- 快代理
- 西祠代理
- www.goubanjia.com

## 代理ip的类型

- **http：**应用到**http协议**对应的**url**之中
- **https：**应用到**https协议**对应的**url**之中

```python
import requests

headers={}
url = 'https://www.baidu.com/s?wd=ip'
page_text = requests.get(url=url, headers=headers, proxies={"https": '60.167.20.2:8888'}).text
with open('./ip.html', 'w', encoding='utf-8') as fp:
    fp.write(page_text)
```

==免费的ip代理不好找了==

# 异步爬虫

使用**异步**来实现**高性能**的数据爬取操作

```python
url = [
    '',
    '',
    ''
]

# 用于获取图片数据
def get_content(url):
    print('正在爬取：' + url)
    # get方法是一个阻塞的方法，一旦url为多个，那么就无法使用多线程工作，降低了效率
    response = requests.get(url=url, headers=headers)
    if response.status_code == 200:
        return response.content

# 用于数据解析
def parse_content(content):
    pass
```

- 线程池、进程池（适度使用：具备上限）

## 线程池

- 导入线程池模块对应的类

  ```python
  from multiprocessing.dummy import Pool
  ```

- 实例化一个线程池对象

  ```python
  pool = Pool(4)
  # 在线程池中开辟了4个线程
  ```

- 使用线程池

  ```python
  url_list = [
      '',
      '',
      '',
      ''
  ]
  
  pool.map(self, func, iterable)
  # 这是这个函数需要传入的参数
  pool = Pool(4)
  pool.map(get_text, url_list)
  # 将列表中的每一个元素都交给第一个参数（也就是方法）中使用，这里使用方法名，不调用该方法
  ```

  