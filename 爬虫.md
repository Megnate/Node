# 案例

代码没有分行显示，可以  ctrl+alt+L 来实现分行处理

## 爬取对应词条的搜索页面

```python
import requests

# 需要UA伪装
# 将对应的User-Agent封装到一个字典当中
# 这个是在Network中检查的，在页面响应的瞬间可以查看
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66'
}
kw = str(input('enter:'))
url = 'https://www.sogou.com/web?query={0}'.format(kw)
response = requests.get(url=url, headers=headers)
page_txt = response.text
with open('./save.html', 'w', encoding='utf-8') as fp:
    fp.write(page_txt)
    # 对于爬取出来之后的代码没有分行显示，可以  ctrl+alt+l 来实现分行处理
    print('end')
```

`requests.get(url,param,headers)`三个参数，其中param可以对应的是**query**的值，也是可以保存在字典当中的

## 爬取翻译网站的文本框中的数据

一般来说定位到XHR中，可以找到该网站所请求的数据包

![image-20201229155100078](C:\Users\梅桂楠\AppData\Roaming\Typora\typora-user-images\image-20201229155100078.png)

明显是一个**POST**的请求，所以需要携带参数

![image-20201229155247743](C:\Users\梅桂楠\AppData\Roaming\Typora\typora-user-images\image-20201229155247743.png)

![image-20201229155430971](C:\Users\梅桂楠\AppData\Roaming\Typora\typora-user-images\image-20201229155430971.png)

**Content-Type：**表示服务端响应为客户端的数据类型，表示相应的数据是一组**JSON数据**

![image-20201229155542856](C:\Users\梅桂楠\AppData\Roaming\Typora\typora-user-images\image-20201229155542856.png)

就是服务器端响应回来的数据

```python
import requests
import json

# 需要UA伪装
# 将对应的User-Agent封装到一个字典当中
# 这个是在Network中检查的，在页面响应的瞬间可以查看，查看了一下，百度和搜狗的这一项是一样的
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66'
}
# 每个网页的url都不同
post_url = 'https://fanyi.baidu.com/sug'
# post请求参数处理，和get是一样的
word = input('enter the word:')
data = {
    'kw': word
}
# 获取请求，还是需要UA伪装
response = requests.post(url=post_url, data=data, headers=headers)
# 获取响应的数据：json()返回的是一个对象obj，检查发现是一个字典对象（如果确认响应数据类型是json类型才可使用）
dic_obj = response.json()
# 持久化存储
with open('./save.json', 'w', encoding='utf-8') as fp:
    # 不能使用ascii码
    json.dump(dic_obj, fp, ensure_ascii=False)
    # 对于爬取出来之后的代码没有分行显示，可以  ctrl+alt+l 来实现分行处理
    print('end')
```

这个相对应的在**有道翻译网页**上是行不通的

## 爬取肯德基位置信息

```python
import requests
import json

# 需要UA伪装
# 将对应的User-Agent封装到一个字典当中
# 这个是在Network中检查的，在页面响应的瞬间可以查看，查看了一下，百度和搜狗的这一项是一样的
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66'
}
# 每个网页的url都不同
cname = str(input('enter:'))
url = 'http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=cname'
data = {
    'cname': cname,
    'pageIndex': '1',   # 从第一个位置开始爬取
    'pageSize': '10'    # 只爬取十个
}
# post请求参数处理，和get是一样的
# 获取请求，还是需要UA伪装
response = requests.post(url=url, data=data, headers=headers)
# 获取响应的数据：json()返回的是一个对象obj，检查发现是一个字典对象（如果确认响应数据类型是json类型才可使用）
dic_obj = response.text
# 持久化存储
with open('./save.json', 'w', encoding='utf-8') as fp:
    # 不能使用ascii码
    json.dump(dic_obj, fp, ensure_ascii=False)
    # 对于爬取出来之后的代码没有分行显示，可以  ctrl+alt+l 来实现分行处理
    print('end')
```

## 爬取化妆品数据

这个主要在于还爬取了化妆品公司的详细信息，有一个多次跳动，也就是对**JSON**文件字段的提取

```python
import requests
import json

# 需要UA伪装
# 将对应的User-Agent封装到一个字典当中
# 这个是在Network中检查的，在页面响应的瞬间可以查看，查看了一下，百度和搜狗的这一项是一样的
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66'
}
# 每个网页的url都不同
url = 'http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do?method=getXkzsList'
data = {
    'on': 'true',
    'page': '1',
    'pageSize': '15',
    'productName': '',
    'conditionType': '1',
    'applyname': '',
    'applysn': ''
}
# post请求参数处理，和get是一样的
# 获取请求，还是需要UA伪装
response = requests.post(url=url, data=data, headers=headers)
# 获取响应的数据：json()返回的是一个对象obj，检查发现是一个字典对象（如果确认响应数据类型是json类型才可使用）
dic_obj = response.json()
# 持久化存储
with open('./save.json', 'w', encoding='utf-8') as fp:
    # 不能使用ascii码
    json.dump(dic_obj, fp, ensure_ascii=False)
    # 对于爬取出来之后的代码没有分行显示，可以  ctrl+alt+l 来实现分行处理
    print('end')

# 从json文件中获取ID的数据
id_list = []
all_data_list = []
for dic in dic_obj['list']:
    id_list.append(dic['ID'])

# 获取详情页的数据
urls = 'http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do?method=getXkzsById'
for ids in id_list:
    datas = {
        'id': ids
    }
    responses = requests.post(url=urls, data=datas, headers=headers)
    # 获取响应的数据：json()返回的是一个对象obj，检查发现是一个字典对象（如果确认响应数据类型是json类型才可使用）
    dic_objs = responses.json()
    all_data_list.append(dic_objs)
with open('./saves.json', 'w', encoding='utf-8') as fp:
    # 不能使用ascii码
    json.dump(all_data_list, fp, ensure_ascii=False)
    # 对于爬取出来之后的代码没有分行显示，可以  ctrl+alt+l 来实现分行处理
    print('end')
```

# 基础

## 聚焦爬虫

爬取页面中指定的页面内容

## 数据解析

通过获取对应的**标签**来获取对应的数据，也可以从**标签**对应的**属性**中获取对应的数据

### 正则

### bs4

### xpath（最常用）

